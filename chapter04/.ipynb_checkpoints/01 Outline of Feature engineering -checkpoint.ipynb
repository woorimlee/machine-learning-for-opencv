{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 특징 엔지니어링의 이해\n",
    "+ 모든 학습 알고리즘에는 장점과 단점이 있지만, 성능 차이는 종종 데이터를 준비하거나 표현하는 방식에 달려 있다.\n",
    "+ 특징 엔지니어링은 질문에 대한 해결책을 배우기 위해 샘플 데이터를 잘 나타내는 방법을 찾는 것이다.\n",
    "+ 특징 엔지니어링은 전처리 단계에 속하며, 주어진 __훈련 데이터__에서 __특징 추출__과정을 거쳐(여기까지 전처리) 머신 러닝 알고리즘 __학습__ 단계 순서로 진행된다.\n",
    "  + __특징 선택__ : 데이터에서 중요한 attribute(특징)를 식별하는 프로세스다. 이미지에서 사용 가능한 특징은 edge, corner, ridge의 위치가 될 수 있다. 이 장에서는 SIFT(Scale-Invariant Feature Transform)와 SURF(Speeded Up Robust Features)같은 OpenCV가 제공하는 고급 feature descriptor를 살펴본다.\n",
    "  + __특징 추출__ : feature space로 원시 데이터를 변환하는 프로세스. 이미지의 코너(선택된 특징)를 추출할 수 있는 Harris(해리스) 연산자가 그 예이다.\n",
    "  \n",
    "### 2 전처리 데이터\n",
    "+ 좋은 결과를 얻기 위한 첫 번째 단계는 __data preprocessing__이다. \n",
    "  1. __Data formatting(형식)__ : 데이터가 작업하기에 적당한 형식이어야 한다. \n",
    "  2. __Data cleaning(정제)__ : 데이터에 유효하지 않거나 누락된 항목이 포함될 수 있으니 정제하거나 제거해야 한다.\n",
    "  3. __Data Sampling(샘플링)__ : 데이터가 너무 방대하면 현명한 방식으로 샘플링 한다.\n",
    "+ 데이터가 전처리 단계를 거치면 __머신 러닝 알고리즘에 맞게 변형__되어야 한다.\n",
    "  1. __Scaling(확장)__ : 종종 데이터가 평균 범위 및 단위 분산을 가지고 일반적인 범위 내에 있어야 한다. 확장은 모든 기능을 일반적인 값 범위로 변경해서 가져오는 프로세스다.\n",
    "  2. __Decomposition(분해)__ : 데이터 세트에는 처리할 수 있는 것보다 더 많은 기능들이 포함되는 경우가 많다. 분해는 좀 더 적은 수의 유익한 데이터 컴포넌트로 데이터를 압축하는 과정이다.\n",
    "  3. __Aggregation(집계)__ : 여러 기능을 하나의 의미 있는 그룹으로 그룹화할 수 있다. \n",
    "\n",
    "  \n",
    "위 프로세스 중 일부 내용에 대해 살펴보도록 하면,\n",
    "  \n",
    "    \n",
    "  1) __Standardizing features(특징 표준화)__ : __standardization__이란 __평균 및 단위 분산이 0__이 되도록 데이터를 확장하는 프로세스를 의미한다.\n",
    "    + 모든 데이터 포인트에서 모든 데이터의 평균값(μ)을 빼고, 이를 데이터 분산(σ)으로 나눔으로써 데이터를 수동으로 표준화 할 수 있다. \n",
    "    \n",
    "    + 즉, 모든 특징 x에 대해 (x - μ) / σ로 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.26726124 -1.33630621  1.33630621]\n",
      " [ 1.33630621  0.26726124 -0.26726124]\n",
      " [-1.06904497  1.06904497 -1.06904497]]\n",
      "[  7.40148683e-17   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# scikit-learn에서 preprocessing 프로세스 구현을 지원함.\n",
    "from sklearn import preprocessing \n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1., -2., 2.], [3., 0., 0.], [0., 1., -1.]])\n",
    "\n",
    "#표준화를 위해서 scale 함수를 사용한다.\n",
    "\n",
    "X_scaled = preprocessing.scale(X)\n",
    "print(X_scaled)\n",
    "\n",
    "# 확장된 데이터 행렬 X_scaled의 평균과 분산을 두 번 검사해 실제로 표준화 됐는지 확인 할 수 있다.\n",
    "\n",
    "print(X_scaled.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \n",
    "  \n",
    "   \n",
    "2) __Normalizing features(특징 정규화)__ : __Normalization__은 개별 샘플을 __unit norm(단위 표준)__으로 __확장__하는 프로세스다. __norm(정규화)__은 __length of a vector(벡터의 길이)__를 의미하고 다른 방법으로 정의될 수 있다. L1 norm(맨해튼 거리)과 L2 norm(유클리드 거리) 두 가지를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2 -0.4  0.4]\n",
      " [ 1.   0.   0. ]\n",
      " [ 0.   0.5 -0.5]]\n",
      "\n",
      " [[ 0.33333333 -0.66666667  0.66666667]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# normalize 함수를 사용해 정규화 할 수 있다.\n",
    "# L1은 norm='l1'으로, L2는 norm='l2'로 지정할 수 있다.\n",
    "\n",
    "X_normalized_l1 = preprocessing.normalize(X, norm='l1')\n",
    "print(X_normalized_l1)\n",
    "\n",
    "X_normalized_l2 = preprocessing.normalize(X, norm='l2')\n",
    "print(\"\\n\", X_normalized_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  \n",
    "\n",
    "3) __Scaling features to a range(특징의 범위 확장)__ : zero mean이나 unit variance(각각 제로 평균, 단위 분산)으로 기능을 확장하는 대신에 특정 최솟값과 최댓값 사이에 특징을 배치하는 방법이 있다. 종종 이 값은 [0, 1] 혹은 [-1, 1]의 값으로 조정되고, 각각 Sigmoid 함수, Tanh 함수의 출력과 비교할 수 있게된다.\n",
    "  + MinMaxScaler를 사용한다.\n",
    "  + 이런 과정을 __min-max scaling(최소-최대 크기 보정)__이라고 한다.\n",
    "  + zero centering(zero mean)과 일반적으로 같이 사용하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33333333  0.          1.        ]\n",
      " [ 1.          0.66666667  0.33333333]\n",
      " [ 0.          1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_min_max = min_max_scaler.fit_transform(X)\n",
    "print(X_min_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본적으로 데이터는 0과 1사이의 크기로 조정된다. feature_range 키워드 인수를 MinMaxScaler constructor에 전달해 다른 범위로 지정할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.33333333 -1.          1.        ]\n",
      " [ 1.          0.33333333 -0.33333333]\n",
      " [-1.          1.         -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range = (-1, 1))\n",
    "X_min_max2 = min_max_scaler.fit_transform(X)\n",
    "print(X_min_max2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  \n",
    "4) __Binarizing features(특징 이진화)__ : 데이터가 실제로 어떤 값을 가지고 있는지에 대해 특별히 관심이 없고, 어떤 기능을 하고 있는지 아닌지에만 관심이 있다면 Threshold 설정으로 데이터 특징을 이진화 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#threshold를 원하는 값으로 설정하자.\n",
    "binarizer = preprocessing.Binarizer(threshold = 0.5)\n",
    "X_binarized = binarizer.transform(X)\n",
    "print(X_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5 이상의 값은 1로 바뀌고, 나머진 0으로 바뀐 것을 확인할 수 있다.\n",
    "\n",
    "####  \n",
    "5) __Handling missing data(누락된 데이터 처리)__ : 특징 엔지니어링에서 또 다른 공통점은 누락된 데이터를 처리하는 것이다. 예를 들어 다음과 같은 데이터 세트를 살펴보자.\n",
    "  + nan은 파이썬에서의 NAN(Not a Number)을 나타낸다. 또한, 대부분의 머신 러닝 알고리즘에서는 이 값을 처리할 수 없기에, 적절한 값으로 대체해야 한다.\n",
    "    + 이를 누락된 값에 대한 imputation(대체)라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "X = np.array([[nan, 0, 3], \n",
    "             [2, 9, -8],\n",
    "             [1, nan, 1],\n",
    "             [5, 2, 4],\n",
    "              [7, -6, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "누락된 값을 대체하기 위한 세 가지 방법은 scikit-learn에서 제공한다.\n",
    "  + 'mean' : 모든 nan 값을 행렬의 지정된 축을 따라 mean value(평균 값)로 바꾼다. 기본 axis = 0.\n",
    "    + The mean = add up all the numbers and then divide by the number of numbers.\n",
    "  + 'median' : 모든 nan 값을 행렬의 지정된 축을 따라 median value(중앙 값)로 바꾼다. 기본 axis = 0.\n",
    "    + The median = the middle value in a list of numbers. To find the median, you need to list the numbers in numerical order, so you may have to rewrite your list first.\n",
    "  + 'most_frequent' : 모든 nan 값을 행렬의 지정된 축을 따라 the most frequent value(가장 빈번한 값)로 바꾼다. 기본 axis = 0.\n",
    "\n",
    "  \n",
    "+ 영어로 적은 부분의 출처. https://www.answerminer.com/blog/use-median-instead-of-mean\n",
    "+ mean 값은 특정 데이터가 극단적으로 한쪽으로 치우쳐져 있는 경우 왜곡될 수 있기 때문에, 작은 수 에서 큰 수로 정렬해 정 가운데 값을 선택하는 median 방식이 존재하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.75  0.    3.  ]\n",
      " [ 2.    9.   -8.  ]\n",
      " [ 1.    1.25  1.  ]\n",
      " [ 5.    2.    4.  ]\n",
      " [ 7.   -6.    3.  ]]\n"
     ]
    }
   ],
   "source": [
    "#mean을 사용한 Imputation\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(strategy = 'mean')\n",
    "X2 = imp.fit_transform(X)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 개의 nan 값이 해당 열을 따라 계산된 평균값과 동일한 채우기 값으로 바뀐다.\n",
    "  \n",
    "   \n",
    "위에서 확인한 X2의 X2[0, 0]인 3.75는 X의 첫 번째 element인 X[0, 0]에 대한 계산을 생략한 채(nan 이니까) 첫 번째 열의 평균을 계산한 값과 동일하다. 아래에서 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75 3.75\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(X[1:, 0]), X2[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   \n",
    "'median'을 적용해보면 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.5  0.   3. ]\n",
      " [ 2.   9.  -8. ]\n",
      " [ 1.   1.   1. ]\n",
      " [ 5.   2.   4. ]\n",
      " [ 7.  -6.   3. ]]\n"
     ]
    }
   ],
   "source": [
    "imp = Imputer(strategy='median')\n",
    "X3 = imp.fit_transform(X)\n",
    "print(X3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X3의 X3[0, 0]은 다음과 같다.\n",
    "\n",
    "  \n",
    "mean에서 했던 방식과 마찬가지로 X[0, 0]에 대한 계산을 생략한 채 중간 값을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5 3.5\n"
     ]
    }
   ],
   "source": [
    "print(np.median(X[1:, 0]), X3[0, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
